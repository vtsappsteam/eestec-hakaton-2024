{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75237b0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "#Load CSV file into DataFrame\n",
    "csv_file = \"ted_talks_en.csv\"\n",
    "fullDataSet = pd.read_csv(csv_file)\n",
    "\n",
    "fullDataSet = fullDataSet.replace(\",\", \"\")\n",
    "\n",
    "with open(\"file_path.txt\", 'w') as file:\n",
    "    file.write(fullDataSet.transcript[2])\n",
    "print(fullDataSet.transcript[2])\n",
    "\n",
    "fullDataSet['topics'] = fullDataSet['topics'].apply(eval)\n",
    "\n",
    "x = fullDataSet[[\"title\", \"description\",\"transcript\"]]\n",
    "y = fullDataSet[\"topics\"]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "  text = text.lower()  # Lowercase\n",
    "  text = re.sub(r\"[^a-z0-9 ]\", \"\", text)  # Remove non-alphanumeric characters\n",
    "  tokens = text.split()  # Tokenize\n",
    "  return tokens\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(set(fullDataSet['topics'].sum())))  # Adjust num_labels based on unique topics\n",
    "\n",
    "def prepare_data(data, label):\n",
    "\n",
    "    tokens_list = []  # Initialize an empty list to hold tokens\n",
    "    for element in data:\n",
    "        preprocessed_text = preprocess_text(element)\n",
    "        tokens_list.extend(preprocessed_text)  # Extend the list with tokens\n",
    "\n",
    "    text = \" \".join(tokens_list)  # Join the tokens with space\n",
    "\n",
    "    encoded_data = tokenizer(text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = encoded_data['input_ids'].squeeze(0)\n",
    "    attention_mask = encoded_data['attention_mask'].squeeze(0)\n",
    "\n",
    "    position_ids = torch.arange(input_ids.size(0))\n",
    "\n",
    "    return input_ids, attention_mask, position_ids, label\n",
    "\n",
    "\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(3):  # Adjust number of epochs\n",
    "  for i in range(len(x_train)):\n",
    "    data = x_train.iloc[i]  # Get entire row (Series)\n",
    "    label = y_train.iloc[i]\n",
    "    input_ids, attention_mask, *other_args = prepare_data(data, label)  # Unpack based on position_ids requirement\n",
    "    \n",
    "    if tokenizer.requires_position_ids:\n",
    "      outputs = model(input_ids, attention_mask=attention_mask, position_ids=other_args[0])  # Use position_ids if required\n",
    "    else:\n",
    "      outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    loss = loss_fn(outputs.logits, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    def prepare_data(data, label):\n",
    "    \"\"\"\n",
    "    Prepares training data for BERT model.\n",
    "    Args:\n",
    "        data: A Pandas Series containing text features (title, description, transcript).\n",
    "        label: A single topic label.\n",
    "    Returns:\n",
    "        A tuple containing tokenized inputs, attention mask, position IDs (if required), and label.\n",
    "    \"\"\"\n",
    "    tokens_list = []  \n",
    "    for element in data:\n",
    "        preprocessed_text = preprocess_text(element)\n",
    "        tokens_list.extend(preprocessed_text)  # Extend the list with tokens\n",
    "\n",
    "    text = \" \".join(tokens_list) \n",
    "\n",
    "    encoded_data = tokenizer(text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = encoded_data['input_ids'].squeeze(0)\n",
    "    attention_mask = encoded_data['attention_mask'].squeeze(0)\n",
    "\n",
    "    if hasattr(model.config, \"position_embedding_type\"):\n",
    "        position_ids = torch.arange(input_ids.size(1))\n",
    "        return input_ids, attention_mask, position_ids, label\n",
    "    else:\n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "\n",
    "# Usage\n",
    "for epoch in range(3):  \n",
    "    for i in range(len(x_train)):\n",
    "        data = x_train.iloc[i] \n",
    "        label = y_train.iloc[i]\n",
    "        *args, label = prepare_data(data, label) \n",
    "        if len(args) == 3:  \n",
    "            input_ids, attention_mask, position_ids = args\n",
    "        else:\n",
    "            input_ids, attention_mask = args\n",
    "\n",
    "        outputs = model(input_ids.unsqueeze(0), attention_mask=attention_mask.unsqueeze(0))\n",
    "        loss = loss_fn(outputs.logits.squeeze(0), label)  # Squeeze the logits tensor to match label shape\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "def prepare_data(data, label):\n",
    "    tokens_list = [] \n",
    "    for element in data:\n",
    "        preprocessed_text = preprocess_text(element)\n",
    "        tokens_list.extend(preprocessed_text)  \n",
    "\n",
    "    text = \" \".join(tokens_list)\n",
    "\n",
    "    encoded_data = tokenizer(text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = encoded_data['input_ids'].squeeze(0)\n",
    "    attention_mask = encoded_data['attention_mask'].squeeze(0)\n",
    "\n",
    "    if hasattr(model.config, \"position_embedding_type\"):\n",
    "        max_length = input_ids.size(0)\n",
    "        position_ids = torch.arange(max_length)\n",
    "        return input_ids, attention_mask, position_ids, label\n",
    "    else:\n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(3):  \n",
    "    for i in range(len(x_train)):\n",
    "        data = x_train.iloc[i] \n",
    "        label = y_train.iloc[i]\n",
    "        *args, label = prepare_data(data, label) \n",
    "        if len(args) == 3:  \n",
    "            input_ids, attention_mask, position_ids = args\n",
    "        else:\n",
    "            input_ids, attention_mask = args\n",
    "\n",
    "        outputs = model(input_ids.unsqueeze(0), attention_mask=attention_mask.unsqueeze(0))\n",
    "        \n",
    "        label_1d = np.array(label).flatten()\n",
    "        \n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoded = label_encoder.fit_transform(label_1d)\n",
    "        label_tensor = torch.tensor(label_encoded)\n",
    "\n",
    "        loss = loss_fn(outputs.logits.squeeze(0), label_tensor)  # Use label_tensor in the loss function\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "def predict_topics(text):\n",
    "  text = preprocess_text(text)\n",
    "  encoded_text, attention_mask = prepare_data([text])\n",
    "  with torch.no_grad():\n",
    "    outputs = model(encoded_text, attention_mask=attention_mask)\n",
    "  predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "  return fullDataSet['topics'].iloc[predicted_label]  # Assuming topics list is indexed the same as labels\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
